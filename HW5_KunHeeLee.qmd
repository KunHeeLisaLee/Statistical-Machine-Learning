---
title: "Statistical Machine Learning - HW5"
author: "Kun Hee (Lisa) Lee (andrewID: kunheel)"
format:
  pdf:
    number-sections: true
    fig-cap: true
    colorlinks: true
    indent: true
    linestretch: 1.15
    fontsize: 12pt
    geometry: "left=1in, right=1in, top=1in, bottom=1in"
---

# Non-Linear Feature Maps

## (a)
```{r}
data <- data.frame(x = c(-3, -2, -1, 1, 4),
                   y = c(1, 1, -1, -1, 1))

plot(data$x, data$y, col = ifelse(data$y == 1, "blue", "red"),
     pch = 20, xlab = "x", ylab = "y")
abline(h=0, lty=2, col="gray")

```

The points are not linearly separable, as it is impossible to find a perfect linear classifier.
As both negative and positive values share same labels, for both -1 and +1, I believe
it means we cannot linearly classify these data points. 

\newpage

# Kernels

Answers on the next page. 

\newpage

# Decision Trees
```{r}
marketing = read.csv('marketing.csv')
head(marketing)

set.seed(1)
idx.test <- sample(1:nrow(marketing), floor(0.3*nrow(marketing)))
test     <- marketing[idx.test, ]
train    <- marketing[-idx.test, ]
```


## (a)
```{r}
library(rpart)
tree <- rpart(y ~ ., data = train, method = "class")
#plot(tree)
```

## (b)
The error says that our fit is not a tree, just a root. I ran `rpart.plot(tree)`
to see how the un-pruned tree looks like, and it showed only one node predicting
"no". By checking through `table(marketing$y)`, I observed that "no" values are
majority of `y`, meaning that because of this class imbalance property, the 
un-pruned tree only showed "no". I believe decision tree made this decision as 
it aims to minimize the misclassification error. 

## (c)
```{r}
wgts <- ifelse(train$y == "yes", 10, 1)

tree_weight <- rpart(y ~ ., data = train, method = "class", weights = wgts)
plot(tree_weight, uniform = TRUE, main = "Weighted Decision Tree", margin = .09)
text(tree_weight, use.n = TRUE, all = TRUE, cex = 0.5)
```


## (d)
```{r}
tree_pred<- predict(tree_weight, test, type = "class")
tree_misclassification <- mean(tree_pred != test$y)

logistic_model <- glm(as.factor(y) ~ ., data = train, family = binomial)
logistic_prob <- predict(logistic_model, test, type = "response")
logistic_pred <- ifelse(logistic_prob > 0.5, "yes", "no")
logistic_misclassification <- mean(logistic_pred != test$y)

base_misclassification <- length(test$y[test$y == "yes"]) / length(test$y)

cat("Decision Tree Misclassification Rate:",
    tree_misclassification, "\n")
cat("Logistic Regression Misclassification Rate:",
    logistic_misclassification, "\n")
cat("Base Misclassification Rate :",
    base_misclassification, "\n")
```

As its shown above, Decision Tree Misclassification Rate is about 0.4, 
while both Logistic Regression Misclassifiation Rate and Base Misclassification
are 0.12. From this, Decision Tree classified not well compared to other methods,
indicating that this might have over-fitted the data or could have not found
the proper splits of the data.

## (e)
```{r}
tree_prob <- predict(tree_weight, test, type = "prob")[, 2]
top_1000 <- order(tree_prob, decreasing = TRUE)[1:1000]
success_rate_top1000 <- mean(test$y[top_1000] == "yes")

cat("Success rate in top_1000:", success_rate_top1000, "\n")
```


## (f)
```{r}
plotcp(tree_weight)
```

```{r}
cp <- 0.039
tree_pruned <- prune(tree_weight, cp = cp)
```

From the cross-validation curve, I chose cp = 0.039 as our optimal cp parameter.
Even though the error was minimized when cp = 0.013, considering its size is
bigger than the one with cp = 0.039 and that the decreased amount of error is 
not that significant compared to the amount from cp = Inf to cp = 0.039, it 
allowed me to choose this as an optimal cp. 

## (g)
```{r}
plot(tree_pruned, uniform = TRUE, main = "Pruned Decision Tree", margin = .09)
text(tree_pruned, use.n = TRUE, all = TRUE, cex = 0.5)
```

```{r}
pruned_pred <- predict(tree_pruned, test, type = "class")
pruned_misclassification_rate <- mean(pruned_pred != test$y)

pruned_prob <- predict(tree_pruned, test, type = "prob")[, 2]
top_1000_pruned <- order(pruned_prob, decreasing = TRUE)[1:1000]
success_rate_top1000_pruned <- mean(test$y[top_1000_pruned] == "yes")

cat("Pruned Decision Tree Misclassification Rate:",
    pruned_misclassification_rate, "\n")
cat("Pruned Success rate (Accuracy) in top 1000:",
    success_rate_top1000_pruned, "\n")
```

