---
title: "Statistical Machine Learning - HW3"
author: "Kun Hee (Lisa) Lee (andrewID: kunheel)"
format:
  pdf:
    number-sections: true
    fig-cap: true
    colorlinks: true
    indent: true
    linestretch: 1.15
    fontsize: 12pt
    geometry: "left=1in, right=1in, top=1in, bottom=1in"
---

```{r, echo=FALSE}
library(tinytex);library(ISLR);library(glmnet);library(ggplot2)
library(reshape2);library(scales)
```

# Regularization

## (a)

We assumed $X = I$ and are trying to solve:

$\hat{\beta}_{LS} = \arg\min_{\beta} \|y - \beta\|^2_2$

Since $X = I$, the least squares objective simplifies to:

$\sum_{j=1}^{n} (y_j - \beta_j)^2$

To find $\beta$, we take the first derivative with respect to $\beta$:

$-2(y - \beta) = 0$

Solving for $\beta$, we obtain:

$\hat{\beta}_{LS}  = y$

## (b)

We assumed $X = I$ and are trying to solve:

$\hat{\beta}_{ridge-pen}(\lambda) = \arg\min_{\beta} \|y - \beta\|^2_2 + \lambda \sum_{j=1}^{n} \beta_j^2$

Since $X = I$, the problem simplifies to:

$\sum_{j=1}^{n} (y_j - \beta_j)^2 + \lambda \sum_{j=1}^{n} \beta_j^2$

Taking the first derivative with respect to $\beta$:

$-2(y_j - \beta_j) + 2 \lambda \beta_j = 0$

Solving for $\beta$, we obtain:

$\beta_j = \frac{y_i}{1 + \lambda}$

Therefore, the ridge regression penalty estimate simplifies to:

$\hat{\beta}_{ridge-pen}(\lambda) = \frac{y}{1+\lambda}$


## (c)

Here, we are trying to solve:

$\hat{\beta}_{ridge-con}(t) = \arg\min_{\beta} \|y - \beta\|^2_2$, subject to 
$\sum_{j=1}^{n} \beta_j^2 <= t$

The loss function $ \|y - \beta\|^2_2$ is minimized at $\beta = y$, which is the

least squares solution we derived in part (a).

Based on the given hint, I drew a ball of radius $\sqrt{t}$, defined by the constraint:

$\sum_{j=1}^{n} \beta_j^2 <= t$

The solution changes based on whether $y$ lies inside or outside this ball:

If $y$ is inside the ball $(||y||_2^2 <= t)$, then the solution is $\beta = y$

If $y$ is outside the ball $(||y||_2^2 > t)$, then the solution is $\hat{\beta}_{ridge-con}(t) = y \frac{\sqrt{t}}{||y||_2}$

Therefore, the final solution for the constrained from ridge regression becomes:

$\hat{\beta}_{ridge-con}(t) = y$, if $(||y||_2^2 <= t)$

and 

$\hat{\beta}_{ridge-con}(t) = y \frac{\sqrt{t}}{||y||_2}$, if $(||y||_2^2 > t)$


## (d)

To show that the two problems from part (b) (penalized form) and part (c) (constrained form) 

are equivalent, we need to relate the penalty parameter $\lambda$ to the constraint radius $t$

From part (b), we derived:

$\hat{\beta}_{ridge-pen}(\lambda) = \frac{y}{1+\lambda}$

The squared norm of this solution is:

$\sum_{j=1}^{n} (\frac{y}{1+\lambda})^2 = \frac{\sum_{j=1}^{n} y_j^2}{(1+\lambda)^2} = \frac{||y||^2_2}{(1+\lambda)^2}$

From part (c), the constraint was:

$\sum_{j=1}^{n} \beta_j^2 <= t$

The equivalence between $\lambda$ and $t$ holds when:

$t = \frac{||y||^2_2}{(1+\lambda)^2}$

Solving for $\lambda$, we get:

$\lambda = \frac{||y||_2}{\sqrt{t}} - 1$

From this, we can check that the penalty parameter $\lambda$ and the constraint 

parameter $t$ are inversely related.


# Regularization and Generalization I.

```{r, echo=FALSE}
## 36-462/662; Assignment 3
##
## Data attaching and splitting script to accompany problem 2

# Install the package with the data in it, if you don't 
# already have it, by un-commenting the following line:
#install.packages('ISLR')

# Load the data we want
data(Hitters, package = 'ISLR')
# Read about the data in the help file
help(Hitters, package = 'ISLR')

# Drop the rows with NA entries for this problem
# (This is, in general, NOT the appropriate way to blindly 
# handle NA entries, but that's a problem for another class)
Hitters <- na.omit(Hitters)

# Set the seed, so you get the same sets I do
set.seed(1)
# Draw 75 samples to be used as a training set
train.idx <- sample(1:nrow(Hitters), 75, replace = FALSE)

# Finally, form the training set (i.e., all the rows we sampled). 
# Also dropping one variable, NewLeague
train <- Hitters[train.idx, -20]
# And form the testing set (i.e., all the rows we did not sample). 
# Again dropping one variable, NewLeague
test <- Hitters[-train.idx, -20]
```


## i.
```{r}
sample_size <- seq(20, 75) # define sample size range

train_set <- list() # list to store train sets

for (n in sample_size) {
  train_set2 <- train[1:n, ]
  train_set[[as.character(n)]] <- train_set2
}

head(train_set[["75"]])
```

## ii. Fitting Linear Regression
```{r}
ls_mse_train <- c()  # vector to store least squares training errors
ls_mse_test <- c() # vector to store least squares testing errors

for (n in sample_size) {
  train_subset <- train[1:n, ]
  lm_model <- lm(Salary ~ ., data = train_subset)
  
  ls_pred_train <- predict(lm_model, train_subset)
  ls_pred_test <- predict(lm_model, test)
  
  ls_mse_train <- c(ls_mse_train, mean((ls_pred_train - train_subset$Salary)^2))
  ls_mse_test <- c(ls_mse_test, mean((ls_pred_test - test$Salary)^2))
}

ls_mse_results <- data.frame(
  Training_Set_Sample_Size = sample_size,
  LS_MSE_Train = ls_mse_train,
  LS_MSE_Test = ls_mse_test
)

print(ls_mse_results)
```

## iii. Fitting Ridge Regression
```{r}
ridge_mse_train <- c()  # vector to store ridge regression training errors
ridge_mse_test <- c() # vector to store ridge regression testing errors
lambda <- 20 # given regularization parameter

for (n in sample_size) {
  train_subset <- train[1:n, ]
  
  X_train <- model.matrix(Salary ~ ., data=train_subset)[, -1]
  X_test <- model.matrix(Salary ~ ., data=test)[, -1]
  
  ridge_model <- glmnet(X_train, train_subset$Salary, alpha = 0, lambda = lambda)
  
  ridge_pred_train <- predict(ridge_model, s = lambda, newx = X_train)
  ridge_pred_test <- predict(ridge_model, s = lambda, newx = X_test)
  
  ridge_mse_train <- c(ridge_mse_train, mean((ridge_pred_train - 
                                                train_subset$Salary)^2))
  ridge_mse_test <- c(ridge_mse_test, mean((ridge_pred_test - 
                                              test$Salary)^2))
}

ridge_mse_results <- data.frame(
  Training_Set_Sample_Size = sample_size,
  Ridge_MSE_Train = ridge_mse_train,
  Ridge_MSE_Test = ridge_mse_test
)

print(ridge_mse_results)
```

Based on the previous tasks, I plotted four curves:
training error and testing error for both least squares and ridge regression.
In the first plot, it was difficult to compare the three curves other than the 
testing error of least squares, due to its high range. To address this, I created
a second plot that limits the y-axis to 1,000,000.

```{r}
mse_results <- data.frame(
  Training_Set_Sample_Size = sample_size,
  LS_MSE_Train = ls_mse_train,
  LS_MSE_Test = ls_mse_test,
  Ridge_MSE_Train = ridge_mse_train,
  Ridge_MSE_Test = ridge_mse_test
)

mse_long <- melt(mse_results, id.vars = "Training_Set_Sample_Size",
                 variable.name = "Model", value.name = "MSE")
mse_long$Model <- factor(mse_long$Model,
                         levels = c("LS_MSE_Train", "LS_MSE_Test",
                                    "Ridge_MSE_Train", "Ridge_MSE_Test"),
                         labels = c("Train(Least Squares)", "Test(Least Squares)",
                                    "Train(Ridge)", "Test(Ridge)"))
```

```{r}
ggplot(mse_long, aes(x=Training_Set_Sample_Size, y=MSE, color=Model)) +
  geom_line(linewidth = 1) +
  labs(title = "Training and Testing MSEs",
       x="Training Set Sample Size (n)",
       y="Mean Squared Error (MSE)") +
  scale_y_continuous(labels = comma)
```

```{r}
ggplot(mse_long, aes(x=Training_Set_Sample_Size, y=MSE, color=Model)) +
  geom_line(linewidth = 1) +
  labs(title = "Training and Testing MSEs (Y up to 1,000,000)",
       x="Training Set Sample Size (n)",
       y="Mean Squared Error (MSE)") +
  scale_y_continuous(labels = comma, limits = c(0, 1000000))
```

Based on the above two plots, I was able to examine features of each curves.

In terms of training error, the second plot allows for a clearer comparison 
between least squares and ridge regression. Both remain low and relatively 
constant, with a slight increase as the sample size grows. However, the least
squares training error appears slightly lower than ridge regression.

In terms of testing error, both plots provide useful insights regarding its behavior.
In the first plot, the testing error for least squares is significantly higher than
ridge regression, likely due to overfitting. The second plot shows that both testing
errors gradually stabilize, though with some fluctuations. This suggests that too 
much regularization in ridge regression can slightly degrade performance.

Overall, the results confirm that ridge regression helps reduce overfitting,
especially for smaller training sizes (n = 20-30), while least squares regression
tends to have higher variance. 


# Regularization and Generalization II.

## (a) Regularization paths
```{r}
X_train <- model.matrix(Salary ~ ., data=train_subset)[, -1] # training data
lambdas <- 10^seq(4, -2, length=100) # log of lambda values

ridge_model <- glmnet(X_train, train$Salary, alpha = 0, lambda = lambdas)
lasso_model <- glmnet(X_train, train$Salary, alpha = 1, lambda = lambdas)
ridge_coef <- as.data.frame(t(as.matrix(coef(ridge_model))[-1, ]))
lasso_coef <- as.data.frame(t(as.matrix(coef(lasso_model))[-1, ]))
ridge_coef$Lambda <- lambdas
lasso_coef$Lambda <- lambdas

ridge_long <- melt(ridge_coef, id.vars = "Lambda", variable.name = "Predictor", 
                   value.name = "Coefficient")
ridge_long$Model <- "Ridge"
lasso_long <- melt(lasso_coef, id.vars = "Lambda", variable.name = "Predictor", 
                   value.name = "Coefficient")
lasso_long$Model <- "LASSO"

coef_results_long <- rbind(ridge_long, lasso_long)
```

```{r}
ggplot(coef_results_long, aes(x=Lambda, y=Coefficient, color=Predictor)) +
  geom_line() +
  facet_wrap(~ Model) +
  labs(title = "Regularization Paths: Ridge vs LASSO",
       x = "log(Lambda)",
       y = "Coefficients") +
  theme(legend.position = "none")
```

From the above plot, I can observe the regularization paths for both LASSO and
Ridge regressions. 
In the left plot (LASSO), some coefficients shrink to exactly zero, indicating 
that LASSO performs variable selection. 
In the right plot (Ridge), coefficients gradually decrease in magnitude but never 
reach zero, meaning that Ridge does not perform variable selection, only coefficient
shrinkage. 

## (b) Test error plots 
```{r}
X_train <- model.matrix(Salary ~ ., data = train)[, -1]
X_test <- model.matrix(Salary ~ ., data = test)[, -1]
lambdas <- 10^seq(4, -2, length = 100)

ridge_model <- glmnet(X_train, train$Salary, alpha = 0, lambda = lambdas)
lasso_model <- glmnet(X_train, train$Salary, alpha = 1, lambda = lambdas)

mse_ridge <- sapply(lambdas, function(lam) {
  pred <- predict(ridge_model, s = lam, newx = X_test)
  mean((pred - test$Salary)^2)
})

mse_lasso <- sapply(lambdas, function(lam) {
  pred <- predict(lasso_model, s = lam, newx = X_test)
  mean((pred - test$Salary)^2)
})

mse_results2 <- data.frame(
  log_lambda = log(lambdas),
  MSE_Ridge = mse_ridge,
  MSE_LASSO = mse_lasso
)

mse_long <- melt(mse_results2, id.vars = "log_lambda",
                 variable.name = "Model", value.name = "MSE")
```

```{r}
ggplot(mse_long, aes(x=log_lambda, y=MSE, color=Model)) +
  geom_line(size = 1) +
  labs(title = "Test MSE vs. Regularization Parameter",
       x = "log(Lambda)", 
       y = "Mean Squared Error (MSE)")
```

From the above plot, I observe the characteristic regularization dip in both  
Ridge and LASSO regression. For Ridge, the test MSE decreases up to log(Lambda) = 7.5,
indicating that regularization improves generalization. However, beyond this point, 
the MSE increases, suggesting that too much regularization leads to under fitting.
For LASSO, the test MSE decreases up to log(Lambda) = 5, then increases until 
log(Lambda) = 6, and finally remains constant.
Although the trends differ slightly between Ridge and LASSO, both models demonstrate
that regularization is helpful up to a certain point, but excessive regularization
degrades performance. 

