---
title: "Statistical Machine Learning - HW6"
author: "Kun Hee (Lisa) Lee (andrewID: kunheel)"
format:
  pdf:
    number-sections: true
    fig-cap: true
    colorlinks: true
    indent: true
    linestretch: 1.15
    fontsize: 12pt
    geometry: "left=1in, right=1in, top=1in, bottom=1in"
---

```{r, echo=FALSE}
library(tinytex);library(rpart)
```

# Boosting in A Simple Example
```{r}
X <- matrix(c(11, 3, 10, 1, 4, 4, 12, 10, 2, 4, 10, 5, 8, 8, 6, 5, 7, 7, 7, 8), 
            ncol=2, byrow=TRUE)
y <- c(-1, -1, -1, +1, -1, +1, -1, +1, +1, +1)

data <- data.frame(x1 = X[,1], x2 = X[,2], y = y)
```


## (a) Initialize weights
```{r}
# Initial weights : 1/N
w_1 <- rep(1/10, 10)
w_1
```

For the initial weights, I assigned it to be uniform : 1/10

```{r}
source('X62_assignment_6_adaboost_helpers.R')
best_learner <- find_split(data, w_1)
best_learner
```


To find the weak-learner that minimizes the weighted training error,
I used a function `find_split` from a helper file. Based on its output, I got to 
know that my weak-learner follows the rule:

Label +1 if X2 > 4.5 and -1 otherwise 


## (b)
```{r}
pred <- predict(best_learner$tree, type = "class")
error <- sum(w_1 * (pred != y)) / sum(w_1)
error
```


As calculated above, the weighted training error of this weak-learner 
is 0.1. 

```{r}
alpha1 <- 0.5 * log((1 - error) / error)
alpha1
```

The corresponding $\alpha_1$ is about 1.099.



## (c) Update weights

Each weight is updated by:

$$w_i^{new} = w_i^{old} \cdot e^{-\alpha_1 y_i h(x_i)}$$

where $h(x_i)$ indicates the prediction.

```{r}
w_new <- w_1 * exp(-alpha1 * as.numeric(y) * as.numeric(pred))
w_new <- w_new / sum(w_new)
w_new
```

Now, the corresponding weak-learner becomes: 

```{r}
best_learner2 <- find_split(data, w_new)
best_learner2
```
This follows the rule:

Label +1 if X1 > 11.5 and -1 otherwise

## (d)

```{r}
pred2 <- predict(best_learner2$tree, type = "class")
error2 <- sum(w_new * (pred2 != y)) / sum(w_new)
error2
```

As calculated from above, training error of the second weak-learner is
about 0.021. Now, I will calculate $\alpha_2$ based on this.

```{r}
alpha2 <- 0.5 * log((1 - error2) / error2)
alpha2
```

The corresponding $\alpha_2$ is about 1.930.


## (e)
So far, I took 2 rounds of boosting. Based on this, to find the final classification
rule produced by AdaBoost, I will combine the two classifiers. As the final
classifier combines the two weak learners using their weights $\alpha_1$ and 
$\alpha_2$:

$$H(x) = sign(\alpha_1 h_1(x) + \alpha_2 h_2(x))$$

Therefore, the final classification rule is:

$$H(x) = sign(1.099 h_1(x) + 1.930 h_2(x))$$

where 

$h_1(x) = +1$ if $X2 > 4.5$, $h_1(x) = -1$ otherwise 

and 

$h_2(x) = +1$ if $X1 > 11.5$, $h_2(x) = -1$ otherwise


# Implementing AdaBoost

## (a) Completing the functions 

I copied the two functions from main.R file.

```{r}
my_adaboost <- function(pts, B = 10) {
  n      <- length(pts$y)
  # Observation weights
  wgts   <- rep(1 / n, n)
  # A list to store the trees as we compute them
  trees  <- vector('list', length = B)
  # A vector to store the tree weights
  alphas <- numeric(B)
  
  for (b in 1:B) {
    split <- find_split(pts, wgts)
    
    tree <- split$tree
    preds <- predict(tree, type = 'class')
    error <- sum(wgts * ((as.numeric(preds)-1) != pts$y)) / sum(wgts)
    alpha <- 0.5 * log((1 - error) / (error + 1e-10))

    wgts <- wgts * exp(alpha * (pts$y != (as.numeric(preds)-1)))
    wgts[is.na(wgts) | wgts < 0] <- 1e-6
    wgts <- wgts / sum(wgts)
    
    trees[[b]] <- tree
    alphas[b]  <- alpha
  }
  
  return(list(trees = trees, alphas = alphas))
}


predict_adaboost <- function(btrees, pts) {
  n        <- length(pts$y)
  answers  <- pts$y
  # Initialize a score vector to zero.  This will store the ongoing sum 
  #  alpha_b fhat_b(x) for each point
  score    <- numeric(n)
  B        <- length(btrees$alphas)
  # This will store the misclassification error at each step,
  #  so we can see how it changes with the number of trees
  test_err <- numeric(B)
  
  for(b in 1:B){
    tree <- btrees$trees[[b]]
    preds <- (as.numeric(predict(tree, newdata=pts, type = 'class'))-1) * 2-1 
    score <- score + btrees$alphas[b] * preds
    pred <- ifelse(score > 0, 1, 0)
    test_err[b] <- mean(pred != pts$y)
  }
  
  return(list(score = score, 
              predictions = as.numeric(score > 0), 
              test_err = test_err))
}
```


## (b) Data 1: Circle Data & Data 2: Swirl Data 
```{r, fig.width=10, fig.height=13.5}
par(mfrow=c(4,2))
get_circle_data <- function(n) {
  X <- matrix(rnorm(2 * n), ncol = 2)
  Y <- as.numeric(X[, 1]^2 + X[, 2]^2 < 1)
  return(list(x1 = X[,1], x2 = X[,2], y = Y))
}

get_swirl_data = function(n) {
  n1 <- floor(n / 2)
  n2 <- n - n1
  
  X <- matrix(0, nrow = n, ncol = 2)
  Y <- matrix(0, nrow = n, ncol = 1)
  
  r <- seq(0, 1, length.out = n1)
  t <- seq(0, 4, length.out = n1) + rnorm(n1) * 0.2
  
  X[1:n1, 1] <- r * sin(t)
  X[1:n1, 2] <- r * cos(t)
  Y[1:n1]    <- 0
  
  r <- seq(0, 1, length.out = n2)
  t <- seq(4, 8, length.out = n2) + rnorm(n2) * 0.2
  
  X[(n1+1):n, 1] <- r * sin(t)
  X[(n1+1):n, 2] <- r * cos(t)
  Y[(n1+1):n]    <- 1
  
  return(list(x1 = X[, 1], x2 = X[, 2], y = as.numeric(Y)))
}

set.seed(1)
train_circle <- get_circle_data(500)
test_circle  <- get_circle_data(500)
train_swirl <- get_swirl_data(500)
test_swirl <- get_swirl_data(500)

plot(train_circle$x1, train_circle$x2,
     xlab = "X1", ylab = "X2", main = "Train Set of Circle Data",
     col = ifelse(train_circle$y == +1, "pink", "lightgreen"))
legend("topright", legend = c("Class: +1", "Class: -1"), 
       col = c("pink", "lightgreen"), pch = 20)
plot(train_swirl$x1, train_swirl$x2,
     xlab = "X1", ylab = "X2", main = "Train Set of Swirl Data",
     col = ifelse(train_swirl$y == +1, "pink", "lightgreen"))
legend("topright", legend = c("Class: +1", "Class: -1"), 
       col = c("pink", "lightgreen"), pch = 20)

for (B in 1:3){
  boosted_circle <- my_adaboost(train_circle, B)
  scores_circle <- predict_adaboost(boosted_circle, train_circle)$score
  draw_boosted_trees(boosted_circle, train_circle, scores_circle)
  
  boosted_swirl <- my_adaboost(train_swirl, B)
  scores_swirl <- predict_adaboost(boosted_swirl, train_swirl)$score
  draw_boosted_trees(boosted_swirl, train_swirl, scores_swirl)
}
```

## (c)
```{r}
boosted_circle_full <- my_adaboost(train_circle, 250)
pred_circle_train <- predict_adaboost(boosted_circle_full, train_circle)
pred_circle_test <- predict_adaboost(boosted_circle_full, test_circle)

boosted_swirl_full <- my_adaboost(train_swirl, 250)
pred_swirl_train <- predict_adaboost(boosted_swirl_full, train_swirl)
pred_swirl_test <- predict_adaboost(boosted_swirl_full, test_swirl)

plot(1:250, pred_circle_train$test_err, type = "l", col = "blue", ylim = c(0, 1),
     xlab = "Number of Trees", ylab = "Error",
     main = "Training and Test Error vs. Trees")
lines(1:250, pred_circle_test$test_err, col = "red")
legend("topright", legend = c("Training Error", "Test Error"),
       col = c("blue", "red"), lty = 1)

plot(1:250, pred_swirl_train$test_err, type = "l", col = "blue", ylim = c(0, 1),
     xlab = "Number of Trees", ylab = "Error",
     main = "Training and Test Error vs. Trees")
lines(1:250, pred_swirl_test$test_err, col = "red")
legend("topright", legend = c("Training Error", "Test Error"),
       col = c("blue", "red"), lty = 1)
```

From the plot above, I can check that my AdaBoost is working well, because
both training and test errors decreased significantly. 
```{r}
draw_boosted_trees(boosted_circle_full, train_circle, pred_circle_train$score)
draw_boosted_trees(boosted_swirl_full, train_swirl, pred_swirl_train$score)
```


