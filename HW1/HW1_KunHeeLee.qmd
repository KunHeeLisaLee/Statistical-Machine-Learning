---
title: "Statistical Machine Learning - HW1"
author: "Kun Hee (Lisa) Lee (andrew ID: kunheel)"
format: html
editor: visual
---

```{r}
library(tinytex);library(ISLR)
```

## 1. Classification when the losses are asymmetric
# (a)
As the losses are asymmetric, I assume that the threshold will not be 0.5.

For f(x) = 1, the expected loss is:

$R( f(x) = 1 ) = P(Y=1 | X=x) \cdot 0 + P(Y=0 | X=x) \cdot L_{01} = P(Y=0 | X=x) \cdot L_{01}$

For f(x) = 0, the expected loss is:

$R( f(x) = 0 ) = P(Y=1 | X=x) \cdot L_{10} + P(Y=0 | X=x) \cdot 0 = P(Y=1 | X=x) \cdot L_{10}$

Based on this, we classify f(x) as 1 if the loss of f(x) = 1 is less than the loss of 
f(x) = 0. 

$-> P(Y=0 | X=x) \cdot L_{01} < P(Y=1 | X=x) \cdot L_{10}$

Substitute $P(Y=0 | X=x) = 1 - P(Y=1 | X=x)$

We can express the above inequality as ${1 - P(Y=1 | X=x)} \cdot L_{01} <  P(Y=1 | X=x) \cdot L_{10}$

$L_{01} - P(Y=1 | X=x) \cdot L_{01} < P(Y=1 | X=x) \cdot L_{10}$

$L_{01} < P(Y=1 | X=x) \cdot (L_{01} + L_{10})$

$P(Y=1 | X=x) > \frac{L_{01}}{L_{01} + L_{10}}$

Therefore, the decision rule is:

$f(x) = 1$ if $P(Y=1 | X=x) > \frac{L_{01}}{L_{01} + L_{10}}$ and 

$f(x) = 0$ otherwise.

# (b)
If $L_{10} > L_{01}$, the threshold becomes $\frac{L_{01}}{L_{01} + L_{10}} < 0.5.$

This means the model classifies f(x) as 1 even when $P(Y=1 | X=x)$ is less than 0.5.

This makes sense because we assumed false negatives are worse (more costly)

than false positives. To minimize the expected loss, the model gets lower 

threshold so that it increases the chance of false positives rather than false 

negatives.


## 2. Drawbacks of using regression to design classifiers 
# (a)
```{r}
data("Default")
head(Default)
```
```{r}
Default$default2 <- as.numeric(Default$default == "Yes")
model <- lm(default2 ~ balance, data=Default)
#summary(model)
plot(Default$balance, Default$default2,
     xlab="Balance", ylab="Probability of Default", 
     main="Linear Regression Fit", col="pink")
abline(model, col="skyblue", lwd=3)
```
From the plot above, I can observe that our prediction can be negative. 

# (b)
Using regression in this case, I would have to assign Y = 2 (drug overdose) because 
2 is the average of the Ys from the given nearest neighbors. However, this prediction
does not make sense because Y = 2 (drug overdose) represents totally different class, 
and there are no nearest points with this label. As learned from the class, regression 
is designed for predicting continuous Ys and assumes numerical relationships between 
values. In this case, the labels Y = 1, 2, and 3 represent discrete and unordered 
categories, so regression is not suitable. Instead, we should predict Ys using 
classification in this case. 
